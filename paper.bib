@misc{AceHighPerformance,
  title = {Ace - {{The High Performance Code Editor}} for the {{Web}}},
  urldate = {2023-08-10},
  howpublished = {https://ace.c9.io/}
}

@inproceedings{blanchardStopReinventingWheel2022,
  title = {Stop {{Reinventing}} the {{Wheel}}! {{Promoting Community Software}} in {{Computing Education}}},
  booktitle = {Proc. 2022 {{Work}}. {{Group Rep}}. {{Innov}}. {{Technol}}. {{Comput}}. {{Sci}}. {{Educ}}.},
  author = {Blanchard, Jeremiah and Hott, John R. and Berry, Vincent and Carroll, Rebecca and Edmison, Bob and Glassey, Richard and Karnalim, Oscar and Plancher, Brian and Russell, Se{\'a}n},
  year = {2022},
  month = dec,
  series = {{{ITiCSE-WGR}} '22},
  pages = {261--292},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3571785.3574129},
  urldate = {2023-08-09},
  abstract = {Historically, computing instructors and researchers have developed a wide variety of tools to support teaching and educational research, including exam and code testing suites and data collection solutions. However, these tools often find limited adoption beyond their creators. As a result, it is common for many of the same functionalities to be re-implemented by different instructional groups within the Computing Education community. We hypothesise that this is due in part to discoverability, availability, and adaptability challenges. Further, instructors often face institutional barriers to deployment, which can include hesitance of institutions to rely on community developed solutions that often lack a centralised authority and may be community or individually maintained. To this end, our working group explored what solutions are currently available, what instructors needed, and the reasons behind the above-mentioned phenomenon. To do so, we reviewed existing literature and surveyed the community to identify the tools that have been developed by the community; the solutions that are currently available and in use by instructors; what features are needed moving forward for classroom and research use; what support for extensions is needed to support further Computing Education research; and what institutional challenges instructors and researchers are currently facing or have faced in using community software solutions. Finally, the working group identified factors that limited adoption of solutions. This work proposes ways to integrate and improve the availability, discoverability, and dissemination of existing community projects, as well as ways to manage and overcome institutional challenges.},
  isbn = {9798400700101},
  keywords = {community software,computing education,computing education research,educational tools,open source software}
}

@misc{CodeAutogradingPlatform,
  title = {Code {{Autograding Platform}} | {{Codequiry}}},
  urldate = {2023-08-10},
  howpublished = {https://codequiry.com/auto-grading-programming}
}

@misc{CodeGradeVirtualAssistant,
  title = {{{CodeGrade}} - {{Virtual Assistant}} for Your Coding Classroom},
  urldate = {2023-08-10},
  abstract = {The Virtual Assistant for your coding classroom. CodeGrade offers automatic grading, rubrics, inline comments, analytics and more. Teach more in less time, improve learning outcomes for students and simplify your code teaching workflow.},
  howpublished = {https://www.codegrade.com/}
}

@misc{CodingRoomsDeveloper,
  title = {Coding {{Rooms}} - {{Developer}} Training and Enablement},
  urldate = {2023-08-10},
  abstract = {Easily create and deliver all levels of interactive coding education \textendash{} from teaching beginners to reskilling professionals \textendash{} and do it at scale, whether you're training ten or ten thousand.},
  howpublished = {https://www.codingrooms.com/}
}

@misc{CodioHandsOnPlatform,
  title = {Codio | {{The Hands-On Platform}} for {{Computing}} \& {{Tech Skills Education}}},
  urldate = {2023-08-10},
  abstract = {Codio is the evidence-based platform for high-quality computing education and tech skills development. Jump-start engagement, boost performance, save time.},
  howpublished = {https://www.codio.com},
  langid = {english}
}

@inproceedings{edwards2008web,
  title = {Web-{{CAT}}: Automatically Grading Programming Assignments},
  booktitle = {Proc. 13th {{Annu}}. {{Conf}}. {{Innov}}. {{Technol}}. {{Comput}}. {{Sci}}. {{Educ}}.},
  author = {Edwards, Stephen H and {Perez-Quinones}, Manuel A},
  year = {2008},
  pages = {328--328}
}

@article{hawlitschekEmpiricalResearchPair2022,
  title = {Empirical Research on Pair Programming in Higher Education: A Literature Review},
  shorttitle = {Empirical Research on Pair Programming in Higher Education},
  author = {Hawlitschek, Anja and Berndt, Sarah and Schulz, Sandra},
  year = {2022},
  month = mar,
  journal = {Comput. Sci. Educ.},
  volume = {0},
  number = {0},
  pages = {1--29},
  publisher = {{Routledge}},
  issn = {0899-3408},
  doi = {10.1080/08993408.2022.2039504},
  urldate = {2023-08-08},
  abstract = {Background and Context Pair programming is an important approach to fostering students' programming and collaborative learning skills. However, the empirical findings on pair programming are mixed, especially concerning effective instructional design.Objective The objective of this literature review is to provide lecturers with systematic knowledge of current research evidences on the implementation of pair programming in courses by outlining the current state of knowledge on the effects of pair programming in higher education and on effective instructional design.Method The results are based on an analysis of 61 articles conducting empirical studies on pair programming in higher education published between 2010 and 2020.Findings Results of studies on the effects of pair programming in comparison with solo programming are to a great extent positive. Regarding instructional design there remain open questions, such as on effective instructional guidance and appropriate task design.Implications We highlight the need for more research on instructional design of pair programming to provide, for example, knowledge about effective procedures for pair programming, effective guidelines, or problem-solving approaches when problems occur within a team. We give recommendations for practitioners based on our findings.},
  keywords = {instructional design,literature review,Pair programming}
}

@misc{OpenAIPlatform,
  title = {{{OpenAI Platform}}},
  urldate = {2023-08-08},
  abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
  howpublished = {https://platform.openai.com},
  langid = {english},
  file = {/Users/srp33/Zotero/storage/8MRNIL3I/chat-completions-api.html}
}

@article{paivaAutomatedAssessmentComputer2022,
  title = {Automated {{Assessment}} in {{Computer Science Education}}: {{A State-of-the-Art Review}}},
  shorttitle = {Automated {{Assessment}} in {{Computer Science Education}}},
  author = {Paiva, Jos{\'e} Carlos and Leal, Jos{\'e} Paulo and Figueira, {\'A}lvaro},
  year = {2022},
  month = jun,
  journal = {ACM Trans. Comput. Educ.},
  volume = {22},
  number = {3},
  pages = {34:1--34:40},
  doi = {10.1145/3513140},
  urldate = {2023-08-10},
  abstract = {Practical programming competencies are critical to the success in computer science (CS) education and go-to-market of fresh graduates. Acquiring the required level of skills is a long journey of discovery, trial and error, and optimization seeking through a broad range of programming activities that learners must perform themselves. It is not reasonable to consider that teachers could evaluate all attempts that the average learner should develop multiplied by the number of students enrolled in a course, much less in a timely, deep, and fair fashion. Unsurprisingly, exploring the formal structure of programs to automate the assessment of certain features has long been a hot topic among CS education practitioners. Assessing a program is considerably more complex than asserting its functional correctness, as the proliferation of tools and techniques in the literature over the past decades indicates. Program efficiency, behavior, and readability, among many other features, assessed either statically or dynamically, are now also relevant for automatic evaluation. The outcome of an evaluation evolved from the primordial Boolean values to information about errors and tips on how to advance, possibly taking into account similar solutions. This work surveys the state of the art in the automated assessment of CS assignments, focusing on the supported types of exercises, security measures adopted, testing techniques used, type of feedback produced, and the information they offer the teacher to understand and optimize learning. A new era of automated assessment, capitalizing on static analysis techniques and containerization, has been identified. Furthermore, this review presents several other findings from the conducted review, discusses the current challenges of the field, and proposes some future research directions.},
  keywords = {Automated assessment,computer science,feedback,learning analytics,programming}
}

@inproceedings{pevelerSubmittyOpenSource2017,
  title = {Submitty: {{An Open Source}}, {{Highly-Configurable Platform}} for {{Grading}} of {{Programming Assignments}} ({{Abstract Only}})},
  shorttitle = {Submitty},
  booktitle = {Proc. 2017 {{ACM SIGCSE Tech}}. {{Symp}}. {{Comput}}. {{Sci}}. {{Educ}}.},
  author = {Peveler, Matthew and Tyler, Jeramey and Breese, Samuel and Cutler, Barbara and Milanova, Ana},
  year = {2017},
  month = mar,
  series = {{{SIGCSE}} '17},
  pages = {641},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3017680.3022384},
  urldate = {2023-08-11},
  abstract = {Submitty (http://submitty.org) is an open source programming assignment submission system from the Rensselaer Center for Open Source Software (RCOS) at Rensselaer Polytechnic Institute (RPI). Students can submit their code via a web interface in a variety of ways, where it is then tested with a highly configurable and customizable automated grader. Students receive immediate feedback from the grader, and can resubmit to correct errors as needed. Through an online interface, TAs can access detailed grading results and supplement the automated scores with manual grading (numeric and written feedback) of overall program structure, good use of comments, reasonable error checking, etc. and any non-programming components of the assignment. The instructor can also configure the system to allow for a configurable late day policy on a per assignment and per student basis. We currently use Submitty in eight different courses (spanning from introductory through advanced topics) serving over 1500 students and 35+ instructors and TAs each week. We will present a range of "case study" assignment configurations in a hands-on demo, going from simple through complex, using a variety of different automated grading methods including per-character and per-line output difference checkers, external unit testing frameworks (such as JUnit), memory debugging tools (Valgrind and DrMemory), code coverage (e.g., Emma), static analysis tools, and custom graders. Submitty can be customized per test case as appropriate to apply resource limits (running time, number of processes, output file size, etc.) and to display or hide from students the program output, autograding results, and testing logs.},
  isbn = {978-1-4503-4698-6},
  keywords = {assessment,automated grading,computer science education,learning approaches,testing}
}
